---
title: "Thesis_Test_Program"
author: "Joshua Idachaba"
date: "1/1/2022"
output: html_document
---
#Data

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Current Working Directory
tryCatch({
  setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
  }, error=function(cond){message(paste("cannot change working directory"))
})



```



```{r DataPrep}
#Packages Used
library(tensorflow)
library(tidyverse)
library(keras)
library(DataExplorer)
library(haven)
library(e1071)
library(caret)
library(ISLR)
library(recipes)
library(mice)
library(doParallel)
library(pROC)
library(stringr)

installr::updateR()

write.csv(combined_rev, "Imputed Dataset-Test.csv")
install_tensorflow()

get_File
#Start Parallelization
# detectCores() #Check the Number of cores on your computer
# num_cores <- 7 #note: you can specify a different number if you want, depending on your preference
# cl <- makePSOCKcluster(num_cores)
# registerDoParallel(cl)


#Import Demographics dataset to start
demographics <- read_xpt("DEMO_J.XPT")
diet_behavior <- read_xpt("DBQ_J.XPT")
combined <- demographics %>% left_join(diet_behavior[c(1,40)], by = "SEQN")


#Join all data by SEQN
# for (filename in list.files(".", pattern = "*J.XPT")) {
#   if (filename == "DBQ_J.XPT"){
#     data <- read_xpt(filename)
#     combined <- combined %>% left_join(data, by = "SEQN")
#   }
# }

#Checking which data sets have the SEQN Identifier column
for (filename in list.files(".", pattern = "*J.XPT")) {
   data <- read_xpt(filename)
   print(filename)
   print(length(data %>% select(contains("SEQN"))))
}

#Look at Missing Data
plot_intro(combined)
summary(combined)
```

```{r miceAnalysis, echo=FALSE}
library(mice)
#MICE Imputation (mice::mice)
a <- Sys.time()
combined_mice <- mice(combined, m = 1, seed = 2022, method = "cart", maxit = 1)
b = Sys.time()
mice_time <- b-a


#MICE Imputation- Parallel (mice::parlmice)
a <- Sys.time()
combined_mice_parallel <- parlmice(combined, m = 18, seed = 2022, method = "cart", n.core = 7, n.imp.core = 2, cluster.seed = 1995, maxit = 5)
b = Sys.time()
parlmice_time <- b-a

print("MICE:", mice_time)
print("Parallel MICE:",parlmice_time )


combined_rev <- complete(combined_mice)
combined_rev_parallel <- complete(combined_mice_parallel)

artb <- plot_missing(combined_rev)
ff <- combined_mice$data

xyplot(combined_mice,CBQ596 ~ INDFMPIR,pch=18,cex=1)
densityplot(combined_mice)

```


```{r MachineLearning}

#Note: parlmice allows for parallelization of MICE imputations across multiple cores. For more information on parlmice, you may visit https://www.rdocumentation.org/packages/mice/versions/3.13.0/topics/parlmice or https://www.gerkovink.com/parlMICE/Vignette_parlMICE.html




combined = combined %>% mutate(CBQ596 = ifelse(is.na(CBQ596) | CBQ596 == 9, 2, CBQ596))
combined$CBQ596 = as.factor(combined$CBQ596)

#Data Preparation
set.seed(1995)
combined_rev <- complete(combined_mice)
combined_rev <- combined_rev %>% select(-c(SDDSRVYR, RIDSTATR, RIDAGEMN)) 
combined <- combined_rev
combined$CBQ596[combined$CBQ596 %in% c(2,9,NA)] <-0
combined <- combined %>% mutate_if(is.character, as.factor)
combined <- combined %>% mutate_if(is.factor, make.names)
combined <- combined %>% mutate_if(is.character, as.factor)
combined$SEQN <- as.factor(as.integer(combined$SEQN))

#

#Separation into Training and Test Sets
data_index <- createDataPartition(combined[["CBQ596"]], p = 0.7, list = FALSE)
data_train <- combined %>% slice(data_index)
data_test <- combined %>% slice(-data_index)



#KNN
target_var <- 'CBQ596'
model_form <- CBQ596 ~ . 
model_type <- 'knn' # model type 'knn' indicates k-nearest neighbor
positive_class <- 1  # Indicates observations reported seeing MyPlate
negative_class <- 0 # Indicates observations reported not seeing MyPlate

model_recipe <- recipe(model_form, data = data_train) %>% 
  step_novel(all_predictors(), -all_numeric()) %>% # This adds a novel factor level if the test data comes with new levels of a factor
  
  step_unknown(all_predictors(), -all_numeric()) %>% # This assigns a missing value in a factor variable to 'unknown'
  
  step_dummy(all_nominal(), -all_outcomes()) %>% # This creates dummy variables for each factor except the dependent variable
  
  step_zv(all_predictors()) # This removes variables containing only a single value in their domain


trControl <- trainControl(method='cv', number = 10, savePredictions = TRUE, classProbs = TRUE, summaryFunction = twoClassSummary)

a <- Sys.time()
knn_combined = train(model_recipe, data= data_train, method = model_type, trControl = trControl, tuneLength = 10, metric = "ROC")
b <- Sys.time()

knn_combined
rf_combined
svm_combined

#Logistic Regression

target_var <- 'CBQ596'
model_form <- CBQ596 ~ . 
model_type <- 'glm' # model type 'glm' indicates logistic regression
positive_class <- "1"  # Indicates observations reported seeing MyPlate
negative_class <- "2" # Indicates observations reported not seeing MyPlate

model_recipe <- recipe(model_form, data = data_train) %>% 
  step_novel(all_predictors(), -all_numeric()) %>%
  step_unknown(all_predictors(), -all_numeric()) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_zv(all_predictors())

trControl <- trainControl(method='cv', number = 10, savePredictions = TRUE, classProbs = TRUE, summaryFunction = twoClassSummary)
a <- Sys.time()
logReg_combined = train(model_recipe, data= data_train, method = model_type, trControl = trControl, tuneLength = 10, metric = "ROC")
b <- Sys.time()

print(b-a)

logReg_combined
summary(data_train)

#LASSO Regression
target_var <- 'CBQ596'
model_form <- CBQ596 ~ . 
model_type <- 'glmnet' # model type 'glmnet' indicates either Ridge Regression (alpha = 0) or LASSO (alpha = 1)

model_recipe <- recipe(model_form, data = data_train) %>% 
  step_novel(all_predictors(), -all_numeric()) %>%
  step_unknown(all_predictors(), -all_numeric()) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_normalize(all_predictors()) %>%
  step_zv(all_predictors())

model_info <- list(target_var,model_form,)

trControl <- trainControl(method = 'cv', number = 10, savePredictions = TRUE, selectionFunction = 'oneSE', summaryFunction = twoClassSummary, classProbs = TRUE)

tGrid <- expand.grid(alpha = c(1), lambda = 10^seq(1, -2, length = 100))

a <- Sys.time()
lasso_combined <- train(model_recipe, data = data_train, method = model_type, trControl = trControl, tuneGrid = tGrid, metric = "ROC")
b <- Sys.time()

lasso_combined
print(b-a)

b[1]
target_var <- 'CBQ596'
  model_form <- CBQ596 ~ .

#Decision Tree
decisionTree_caret <- function(data_train,model_info,recipe)
  
  model_type <- 'rpart' # model type 'glmnet' indicates either Ridge Regression (alpha = 0) or LASSO (alpha = 1)
  
  trControl <- trainControl(method = 'cv', number = 10, savePredictions = TRUE, classProbs = TRUE, summaryFunction = twoClassSummary)
  
  model_recipe <- recipe(model_form, data = data_train) %>% 
  step_novel(all_predictors(), -all_numeric()) %>%
  step_unknown(all_predictors(), -all_numeric()) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_zv(all_predictors())
  
  a <- Sys.time()
  decisionTree_combined <- train(model_recipe, data = data_train, method = model_type, trControl = trControl, metric = 'ROC', tuneLength = 10)
  b <- Sys.time()
  print(b-a)

decisionTree_combined

#Random Forest
set.seed(1995)
combined <- combined_rev
combined = combined %>% mutate(CBQ596 = ifelse(is.na(CBQ596) | CBQ596 == 9, 2, 1))
combined$CBQ596 <- as.character(combined$CBQ596)
combined <- combined %>% mutate_if(is.character, as.factor)
combined <- combined %>% mutate_if(is.factor, make.names)
combined <- combined %>% mutate_if(is.character, as.factor)


class(combined$CBQ596)
unique(combined$CBQ596)
data_index <- createDataPartition(combined[["CBQ596"]], p = 0.7, list = FALSE)
data_train <- combined %>% slice(data_index)
data_test <- combined %>% slice(-data_index)
data_test[[target_var]]

target_var <- 'CBQ596'
model_form <- CBQ596 ~ . 
model_type <- 'rf' # model type 'rf' indicates random forest
positive_class <- "X1"  # Indicates observations reported seeing MyPlate
negative_class <- "X2" # Indicates observations reported not seeing MyPlate

as.factor("2")

model_recipe <- recipe(model_form, data = data_train) %>% 
  step_novel(all_predictors(), -all_numeric()) %>%
  step_unknown(all_predictors(), -all_numeric()) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_zv(all_predictors())

# 10-fold cross validation on the training set 
trControl <- trainControl(method = 'cv', savePredictions = TRUE, classProbs = TRUE, summaryFunction = twoClassSummary)

tGrid <- expand.grid(mtry = c(1:25))

#Perform Random Forest
a <- Sys.time()
library(caret)
library(recipes)
rf_combined <- train(model_recipe, data = data_train, method = model_type, trControl = trControl, tGrid = tGrid, metric = 'ROC')
b <- Sys.time()
print(b-a)

summary(rf_combined)
unique(rf_combined_pred)
data_test[[target_var]]

rf_combined_pred <- rf_combined %>% predict(data_test, type = "raw")
confusionMatrix(rf_combined_pred, data_test[[target_var]], positive = positive_class)
class(data_test[[target_var]])

unique(data_test[[target_var]])
unique(rf_combined_pred)

rf_combined_probs <- rf_combined %>% predict(data_test, type = "prob")
rf_combined_roc <- roc(data_test[[target_var]], rf_combined_probs[, positive_class], plot = TRUE, print.auc = TRUE, legacy.axes = TRUE, levels = c(negative_class, positive_class))

#SVM
combined_rev <- complete(combined_mice)
combined_rev <- combined_rev %>% select(-c(SDDSRVYR, RIDSTATR, RIDAGEMN)) 
combined <- combined_rev
data_index <- createDataPartition(combined[["CBQ596"]], p = 0.7, list = FALSE)
data_train <- combined %>% slice(data_index)
data_test <- combined %>% slice(-data_index)

target_var <- 'CBQ596'
model_form <- CBQ596 ~ . 
model_type <- 'svmLinear' # model type 'svmLinear' indicates Support Vector Machine (SVM)
positive_class <- "X1"  # Indicates observations reported seeing MyPlate
negative_class <- "X2" # Indicates observations reported not seeing MyPlate

trControl <- trainControl(method = 'cv', savePredictions = TRUE, classProbs = TRUE, summaryFunction = twoClassSummary)

tGrid <- expand.grid(C = c(0.001, 0.01, 0.1, 1,5,10,50))

# note: use standardization when building the model!
model_recipe <- recipe(model_form, data = data_train) %>% 
  step_novel(all_predictors(), -all_numeric()) %>%
  step_unknown(all_predictors(), -all_numeric()) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_zv(all_predictors())

a <- Sys.time()
svm_combined <- train(model_recipe, data = data_train, method = model_type,  trControl = trControl,tuneGrid = tGrid , metric = 'ROC')
b <- Sys.time()
print(b-a)

a <- Sys.time()
svm_combined_tLength <- train(model_recipe, data = data_train, method = model_type, trControl = trControl, tuneLength = 50 , metric = 'ROC')
b <- Sys.time()
print(b-a)

svm_combined


stopImplicitCluster()
```
```{r correlationAnalysis, echo=FALSE}
data <- combined_rev
cor_testing <- cor(data, method = "spearman")
rownames(cor_testing)
corrplot::corrplot(cor_testing)
summary(cor_testing[1,2])

which.names <- function(DF, value){
   ind <- which(DF > value & DF < 1, arr.ind=TRUE)
   print(paste(rownames(DF)[ind[,"row"]],  colnames(DF)[ind[,"col"]], sep=', '))
}

which.names.strCount <- function(DF, i, value){
   ind <- which(str_count(DF, pattern = i) == value, arr.ind=TRUE)
   print(DF[ind])
}

dupe_rows_cols <- function(list) {
  dupes <- 
  return()
}

m <- lapply(a, as.character)

a <- which.names(cor_testing_df, 0.7)
a <- colnames()

m <- lapply(a, strsplit, split = ", " )

a <- a[-1]

n <- lapply(m,)

which.names(a, str_count("SEQN"))
print(foreach::foreach(a))
```
``` {r miceAnalysis, echo=FALSE}
#MICE Imputation (mice::mice)
library(mice)
a <- Sys.time()
combined_mice <- mice(combined, m = 5, seed = 2022, method = "cart", maxit = 40)
b = Sys.time()
print("MICE")
print(b-a)

alp <- head(combined_rev, 5)
alp_tensor <- as_tensor(alp)

```

**Create & Evaluate Neural Network Using Train & Test Data as Tensor**

Note: As of 1-21-2022, feature_spec() function is not producing an output with dense features, resulting in an error when using both TensorFlow Tensor and data frame inputs. Further investigation of datasets, code, and installed software and packages is needed. 

```{r NeurNetTens, echo = FALSE}
#Create Tensor from Imputed Dataset
library(tensorflow)
library(keras)
library(tfruns)
library(tfdatasets)
library(tidyverse)
install.packages("keras")

#Install TensorFlow and Keras (if necessary)
install_tensorflow()
install_keras()

#Use below command to check that Tensorflow is installed and operating correctly
#tf$print(tf$constant("Hello Tensorflow!"))

train_data <- data_train
test_data <- data_test


#SEQN column needs to be converted from numeric to integer, otherwise SEQN = 100000 will be interpreted as 1e+05 in iter_next(), resulting in an InvalidArgumentError

class(train_data$SEQN)



write.csv(train_data, "train_data.csv", row.names = FALSE)
write.csv(test_data, "test_data.csv", row.names = FALSE)
train_dataset <- make_csv_dataset("train_data.csv", field_delim = ",", batch_size = 5, num_epochs = 1)
test_dataset <- make_csv_dataset("test_data.csv", field_delim = ",", batch_size = 5, num_epochs = 1)

train_dataset %>% 
  reticulate::as_iterator() %>%
  reticulate::iter_next() %>%
  reticulate::py_to_r()
train_data$CBQ596

spec <- feature_spec(train_dataset, CBQ596 ~ .) %>% # Make sure formula has correct target variable!
  step_numeric_column(all_numeric(), normalizer_fn = scaler_standard()) %>%
  step_categorical_column_with_vocabulary_list(all_nominal()) %>%
  step_indicator_column(all_nominal())

spec <- fit(spec)
layer <- layer_dense_features(feature_columns = dense_features(spec))
train_dataset %>% 
  reticulate::as_iterator() %>% 
  reticulate::iter_next() %>% 
  layer()

model <- keras_model_sequential() %>% 
  layer_dense_features(feature_columns = dense_features(spec)) %>% 
  layer_dense(units = 128, activation = "relu") %>% 
  layer_dense(units = 128, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")

model %>% compile(
  loss = "binary_crossentropy",
  optimizer = "adam",
  metrics = "accuracy"
)


model %>% 
  fit(
    train_dataset %>% dataset_use_spec(spec) %>% dataset_shuffle(500),
    epochs = 20,
    validation_data = test_dataset %>% dataset_use_spec(spec),
    verbose = 2
  )

model %>% evaluate(test_dataset %>% dataset_use_spec(spec), verbose = 0)

# As of 1-21-2022, any attempt to add steps to spec results in following error:

#Error in feature_types[i] <- dtype_chr(self$column_types[[which(self$column_names ==  : 
  #replacement has length zero

#Note (1-21-2022 8AM CST): spec can work with data.frame or Tensorflow objects, as per https://tensorflow.rstudio.com/guide/tfdatasets/feature_spec/, so writing to csv and reinserting using make_csv_dataset() is not necessary

#Interestingly, the above error does not occur when using the train data in data.frame as argument for spec. Perhaps the dataset as a tensorflow object is somehow not a valid input?

#Update: 1-21-2022 11:55AM CST: Answer to above question- Yes, inasmuch as the train and test .csv files both contained a vector of empty name with row indices, which cannot be passed in feature_spec() steps as a valid feature name. Make sure that "row.names = FALSE" is included within write.csv() before running. Furthermore, Tensorflow Tensor datasets are preferred to dataframes when running feature_spec(), as Tensorflow datasets act as iterators.


```
**Create and Evaluate Neural Network using Tensorflow for Python- 1st try**

 
```{python NeurNetTens}
"Modules Used"
import tensorflow as tf
import numpy as np
import pandas as pd
from tensorflow.keras import layers

os.listdir(".")

train_data = pd.read_csv("train_data.csv")
test_data = pd.read_csv("test_data.csv")

train_features = train_data.copy()
train_labels = train_features.pop("CBQ596")

test_features = test_data.copy()

for name, column in train_features.items():
  dtype = column.dtype
  if dtype == object:
    dtype = tf.string
  
for i in range(5):
  print(i)
  

model = tf.keras.Sequential([
  layers.Dense(64),
  layers.Dense(1)
])

model.compile(loss = tf.losses.MeanSquaredError(),
              optimizer = tf.optimizers.Adam())

model.fit(train_features, train_labels, epochs = 10)
result = model.evaluate(test_features)

"""Note: As of 1-22-2022, the model can be built, but not evaluated, and it gives the following error:
ValueError: Exception encountered when calling layer "sequential_1" (type Sequential).
    
    Input 0 of layer "dense_2" is incompatible with the layer: expected axis -1of input shape to have value 43, but received input with shape (None, 44)
    
    This may be similar in nature to the errors encountered in Tensorflow for R, as it includes a NoneType input. Further investigation of the dataset is needed."""

```
```{r ManagePython}

#Install packages
reticulate::py_install(c("matplotlib","numpy","pandas"))

```

**Create and Evaluate Neural Network using Tensorflow for Python- 2n try**
```{python NeurNetTens2}
import numpy as np
import tensorflow as tf
import matplotlib
import math
import time
import sklearn
import keras

```
**Create & Evaluate Neural Network Using Train & Test Data as Data Frame**

Note: As of 1-21-2022, feature_spec() function is not producing an output with dense features, resulting in an error when using both TensorFlow Tensor and data frame inputs. Further investigation of datasets, code, and installed software and packages is needed. 

Update 1-21-2022 5:45PM CST - Same error message is showing after a clean reinstall of "reticulate" package from GitHub
```{r NeurNetDF, echo=FALSE}
###################################
#Create Tensor from Imputed Dataset
###################################

#Packages Used
library(tensorflow)
library(keras)
library(tfruns)
library(tfdatasets)
library(tfestimators)
library(reticulate)


install_python(version = "3.8.10")

#Install TensorFlow and Keras (if necessary)
install_tensorflow()
install_keras()

#Training and Test Data
train_data <- read.csv()
test_data <- data_test
reticulate::insta


#Preprocessing Steps
spec_df <- feature_spec(train_data, CBQ596 ~ .) 
spec_df <- spec_df %>% 
  step_numeric_column(has_type("int32"), normalizer_fn = scaler_standard()) %>%
  step_categorical_column_with_vocabulary_list(all_nominal()) %>%
  step_indicator_column(all_nominal())

spec <- fit(spec)
class(spec)
layer <- dense_features(spec)
class(dense_features(spec))

spec_df <- fit(spec_df)
model <- keras_model_sequential() %>% 
  layer_dense_features(feature_columns = dense_features(spec_df)) %>% 
  layer_dense(units = 128, activation = "relu") %>% 
  layer_dense(units = 128, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")

#As of 1-21-2022 12:02pm CST, we have reverted back to using, as the 


```

```{r ExampleSetTens, echo=FALSE}
#Packages Used
library(tensorflow)
library(keras)
library(tfruns)
library(tfdatasets)
library(tfestimators)


TRAIN_DATA_URL <- "https://storage.googleapis.com/tf-datasets/titanic/train.csv"
TEST_DATA_URL <- "https://storage.googleapis.com/tf-datasets/titanic/eval.csv"

train_file_path <- get_file("train.csv", TRAIN_DATA_URL)
test_file_path <- get_file("eval.csv", TEST_DATA_URL)

train_dataset <- make_csv_dataset(
  train_file_path, 
  field_delim = ",",
  batch_size = 5, 
  num_epochs = 1
)

test_dataset <- train_dataset <- make_csv_dataset(
  test_file_path, 
  field_delim = ",",
  batch_size = 5, 
  num_epochs = 1
)

train_dataset %>% 
  reticulate::as_iterator() %>% 
  reticulate::iter_next() %>% 
  reticulate::py_to_r()

spec <- feature_spec(train_dataset, survived ~ .) %>% 
  step_numeric_column(all_numeric(), normalizer_fn = scaler_standard()) %>% 
  step_categorical_column_with_vocabulary_list(all_nominal()) %>% 
  step_indicator_column(all_nominal())

spec <- fit(spec)
layer <- layer_dense_features(feature_columns = dense_features(spec))
train_dataset %>% 
  reticulate::as_iterator() %>% 
  reticulate::iter_next() %>% 
  layer()

model <- keras_model_sequential() %>% 
  layer_dense_features(feature_columns = dense_features(spec)) %>% 
  layer_dense(units = 128, activation = "relu") %>% 
  layer_dense(units = 128, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")

model %>% compile(
  loss = "binary_crossentropy",
  optimizer = "adam",
  metrics = "accuracy"
)
devtools::install_github("rstudio/reticulate")

model %>% 
  fit(
    train_dataset %>% dataset_use_spec(spec) %>% dataset_shuffle(500),
    epochs = 20,
    validation_data = test_dataset %>% dataset_use_spec(spec),
    verbose = 2
  )

model %>% evaluate(test_dataset %>% dataset_use_spec(spec), verbose = 0)

reticulate_python_versions()
?reticulate::use_python()

devtools::install_github("rstudio/reticulate")
remotes::install_github("rstudio/reticulate")
```

```{python NLTKtest}
import nltk



```


```{r setupPython, echo=FALSE}
Sys.setenv(RETICULATE_PYTHON = "~/.virtualenvs/test/Scripts/python.exe")

reticulate::repl_python("~/.virtualenvs/test/Scripts/python.exe")

reticulate::use_python("/c/Users/Joshua/AppData/Local/Microsoft/WindowsApps/python3")
library(reticulate)
virtualenv_create("test2")
use_virtualenv("test")
```